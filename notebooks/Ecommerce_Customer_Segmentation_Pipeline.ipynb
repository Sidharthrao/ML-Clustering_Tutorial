{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# E-commerce Customer Segmentation - Complete ML Pipeline\n",
        "\n",
        "This notebook demonstrates the complete end-to-end unsupervised learning pipeline for customer segmentation, combining all modular components in execution sequence.\n",
        "\n",
        "## Project Overview\n",
        "- **Task**: Unsupervised clustering for customer segmentation\n",
        "- **Dataset**: E-commerce behavioral data from SQLite database (1M events, 146K users)\n",
        "- **Algorithms**: K-means, DBSCAN, Hierarchical clustering\n",
        "- **Goal**: Identify distinct customer groups based on purchasing patterns and behavior\n",
        "\n",
        "## Workflow Overview:\n",
        "1. **Setup and Imports** - Import libraries and configure project\n",
        "2. **Configuration** - Set up directories and verify settings\n",
        "3. **Data Loading** - Extract event-level data from SQLite database\n",
        "4. **Exploratory Data Analysis** - Understand data distribution and patterns\n",
        "5. **Data Aggregation** - Aggregate events to customer-level features\n",
        "6. **Feature Engineering** - Create RFM, behavioral, and temporal features\n",
        "7. **Feature Selection** - Select features suitable for clustering\n",
        "8. **Preprocessing** - Handle outliers, scale, and transform data\n",
        "9. **Optimal Cluster Selection** - Determine best number of clusters\n",
        "10. **Clustering Model Training** - Train and compare multiple algorithms\n",
        "11. **Cluster Evaluation** - Evaluate clustering quality\n",
        "12. **Cluster Analysis** - Profile and interpret each cluster\n",
        "13. **Visualizations** - Create comprehensive visualizations\n",
        "14. **Summary** - Review findings and business insights\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Setup and Imports\n",
        "\n",
        "**Justification**: Import all necessary libraries and set up the project structure. This ensures all dependencies are available and the project root is in the Python path for module imports. We use pandas for data manipulation, sklearn for clustering algorithms, and matplotlib/seaborn for visualizations.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import system and path utilities\n",
        "import sys\n",
        "from pathlib import Path\n",
        "\n",
        "# Set project root for imports\n",
        "project_root = Path().absolute().parent.parent\n",
        "sys.path.insert(0, str(project_root))\n",
        "\n",
        "# Standard libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import joblib\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Visualization\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "plt.style.use('seaborn-v0_8')\n",
        "sns.set_palette(\"Set2\")\n",
        "\n",
        "# Machine Learning - Clustering\n",
        "from sklearn.cluster import KMeans, DBSCAN, AgglomerativeClustering\n",
        "from sklearn.metrics import silhouette_score, davies_bouldin_score, calinski_harabasz_score\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "# Project modules\n",
        "from config.config import (\n",
        "    ensure_directories, MODEL_PATHS, DIRECTORIES, \n",
        "    DATABASE_PATH, DB_TABLE_NAME, DATA_CONFIG,\n",
        "    KMEANS_CONFIG, PREPROCESSING_CONFIG, FEATURE_CONFIG\n",
        ")\n",
        "from src.data.data_loader import load_and_prepare_data\n",
        "from src.data.data_aggregator import aggregate_data\n",
        "from src.preprocessing.feature_engineering import engineer_features, select_features_for_clustering\n",
        "from src.preprocessing.preprocessor import ClusteringPreprocessor\n",
        "from src.clustering.cluster_selector import ClusterSelector\n",
        "from src.clustering.cluster_trainer import ClusteringTrainer\n",
        "from src.evaluation.cluster_evaluator import ClusterEvaluator\n",
        "from src.utils.visualizations import (\n",
        "    plot_pca_clusters, plot_cluster_comparison, plot_feature_distributions_by_cluster,\n",
        "    plot_correlation_matrix, plot_elbow_curve, plot_silhouette_scores\n",
        ")\n",
        "from src.utils.logger import setup_logger\n",
        "\n",
        "print(\"✓ All imports successful\")\n",
        "print(f\"Project root: {project_root}\")\n",
        "print(f\"Database path: {DATABASE_PATH}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Configuration Setup\n",
        "\n",
        "**Justification**: Ensure all necessary directories exist (models, logs, reports, etc.) and verify configuration. This is critical for saving artifacts and logs throughout the pipeline. We also set up logging to track the pipeline execution.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create all necessary directories\n",
        "ensure_directories()\n",
        "\n",
        "# Setup logger\n",
        "logger = setup_logger(\"notebook_pipeline\")\n",
        "\n",
        "# Display configuration\n",
        "print(\"Configuration:\")\n",
        "print(f\"  - Database table: {DB_TABLE_NAME}\")\n",
        "print(f\"  - Chunk size: {DATA_CONFIG.get('chunk_size', 100000):,}\")\n",
        "print(f\"  - K-means n_clusters range: {KMEANS_CONFIG.get('n_clusters_range', [2, 10])}\")\n",
        "print(f\"  - Preprocessing scaling: {PREPROCESSING_CONFIG.get('scaling_method', 'robust')}\")\n",
        "print(f\"  - Use PCA: {PREPROCESSING_CONFIG.get('use_pca', False)}\")\n",
        "print(\"\\n✓ Configuration loaded\")\n",
        "print(f\"✓ Directories created: {list(DIRECTORIES.values())}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Data Loading\n",
        "\n",
        "**Justification**: Load event-level data from the SQLite database. The data loader handles:\n",
        "- Database connection with chunking for memory efficiency (1M+ rows)\n",
        "- Type conversion (event_time to datetime, numeric columns)\n",
        "- Data validation (missing columns, invalid values)\n",
        "- Basic statistics logging\n",
        "\n",
        "This is the raw event-level data where each row represents a user interaction (view, cart, purchase).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load data from database\n",
        "print(\"Loading event-level data from database...\")\n",
        "print(f\"This may take a few minutes for large datasets (1M+ events)...\")\n",
        "\n",
        "df = load_and_prepare_data(\n",
        "    db_path=DATABASE_PATH,\n",
        "    table_name=DB_TABLE_NAME,\n",
        "    chunk_size=DATA_CONFIG.get(\"chunk_size\", 100000),\n",
        "    max_rows=None  # Load all data\n",
        ")\n",
        "\n",
        "print(f\"\\n✓ Data loaded successfully\")\n",
        "print(f\"  - Shape: {df.shape}\")\n",
        "print(f\"  - Columns: {list(df.columns)}\")\n",
        "print(f\"\\nFirst few rows:\")\n",
        "df.head()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Exploratory Data Analysis (EDA)\n",
        "\n",
        "**Justification**: Understanding the data is crucial before modeling. EDA helps identify:\n",
        "- Event type distribution (view, cart, purchase funnel)\n",
        "- Temporal patterns (hourly activity, peak times)\n",
        "- User behavior patterns (engagement levels, spending)\n",
        "- Data quality issues (missing values, outliers)\n",
        "- Product and category preferences\n",
        "\n",
        "This analysis informs feature engineering and helps understand the business context.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Basic statistics\n",
        "print(\"=== Data Overview ===\")\n",
        "print(f\"Total events: {len(df):,}\")\n",
        "print(f\"Date range: {df['event_time'].min()} to {df['event_time'].max()}\")\n",
        "\n",
        "print(f\"\\n=== Missing Values ===\")\n",
        "missing = df.isnull().sum()\n",
        "print(missing[missing > 0] if missing.sum() > 0 else \"No missing values\")\n",
        "\n",
        "print(f\"\\n=== Event Type Distribution ===\")\n",
        "event_counts = df['event_type'].value_counts()\n",
        "print(event_counts)\n",
        "print(f\"\\nEvent type proportions:\")\n",
        "print(df['event_type'].value_counts(normalize=True))\n",
        "\n",
        "# Visualize event type distribution\n",
        "plt.figure(figsize=(10, 6))\n",
        "event_counts.plot(kind='bar', color='steelblue', alpha=0.7)\n",
        "plt.title('Event Type Distribution', fontsize=14, fontweight='bold')\n",
        "plt.xlabel('Event Type')\n",
        "plt.ylabel('Count')\n",
        "plt.xticks(rotation=45)\n",
        "plt.grid(axis='y', alpha=0.3)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Calculate conversion funnel\n",
        "if 'view' in event_counts.index and 'cart' in event_counts.index and 'purchase' in event_counts.index:\n",
        "    view_to_cart = (event_counts.get('cart', 0) / event_counts.get('view', 1)) * 100\n",
        "    cart_to_purchase = (event_counts.get('purchase', 0) / event_counts.get('cart', 1)) * 100\n",
        "    view_to_purchase = (event_counts.get('purchase', 0) / event_counts.get('view', 1)) * 100\n",
        "    \n",
        "    print(f\"\\n=== Conversion Funnel ===\")\n",
        "    print(f\"View → Cart: {view_to_cart:.2f}%\")\n",
        "    print(f\"Cart → Purchase: {cart_to_purchase:.2f}%\")\n",
        "    print(f\"View → Purchase: {view_to_purchase:.2f}%\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Temporal patterns\n",
        "print(\"=== Temporal Analysis ===\")\n",
        "df['hour'] = df['event_time'].dt.hour\n",
        "df['day_of_week'] = df['event_time'].dt.day_name()\n",
        "\n",
        "# Hourly activity\n",
        "hourly_activity = df.groupby('hour').size()\n",
        "print(f\"\\nPeak activity hour: {hourly_activity.idxmax()} (hour {hourly_activity.idxmax()})\")\n",
        "\n",
        "# Visualize hourly activity\n",
        "plt.figure(figsize=(12, 6))\n",
        "hourly_activity.plot(kind='line', marker='o', linewidth=2, markersize=8)\n",
        "plt.title('Hourly Activity Pattern', fontsize=14, fontweight='bold')\n",
        "plt.xlabel('Hour of Day')\n",
        "plt.ylabel('Number of Events')\n",
        "plt.grid(alpha=0.3)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Event types by hour\n",
        "plt.figure(figsize=(14, 6))\n",
        "hourly_by_type = df.groupby(['hour', 'event_type']).size().unstack(fill_value=0)\n",
        "hourly_by_type.plot(kind='bar', stacked=False, width=0.8)\n",
        "plt.title('Event Types by Hour', fontsize=14, fontweight='bold')\n",
        "plt.xlabel('Hour of Day')\n",
        "plt.ylabel('Number of Events')\n",
        "plt.legend(title='Event Type')\n",
        "plt.xticks(rotation=0)\n",
        "plt.grid(axis='y', alpha=0.3)\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# User and product statistics\n",
        "print(\"=== User and Product Statistics ===\")\n",
        "print(f\"Unique users: {df['user_id'].nunique():,}\")\n",
        "print(f\"Unique products: {df['product_id'].nunique():,}\")\n",
        "print(f\"Unique categories: {df['category_id'].nunique():,}\")\n",
        "print(f\"Unique sessions: {df['user_session'].nunique():,}\")\n",
        "\n",
        "# User engagement distribution\n",
        "user_event_counts = df.groupby('user_id').size()\n",
        "print(f\"\\n=== User Engagement Distribution ===\")\n",
        "print(f\"Average events per user: {user_event_counts.mean():.2f}\")\n",
        "print(f\"Median events per user: {user_event_counts.median():.2f}\")\n",
        "print(f\"Max events per user: {user_event_counts.max()}\")\n",
        "\n",
        "# Visualize user engagement\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.hist(user_event_counts, bins=50, color='steelblue', alpha=0.7, edgecolor='black')\n",
        "plt.title('User Engagement Distribution (Events per User)', fontsize=14, fontweight='bold')\n",
        "plt.xlabel('Number of Events')\n",
        "plt.ylabel('Number of Users')\n",
        "plt.yscale('log')\n",
        "plt.grid(alpha=0.3)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Spending statistics (for purchases)\n",
        "purchase_data = df[df['event_type'] == 'purchase']\n",
        "if len(purchase_data) > 0:\n",
        "    print(f\"\\n=== Purchase Statistics ===\")\n",
        "    print(f\"Total purchases: {len(purchase_data):,}\")\n",
        "    print(f\"Average order value: ${purchase_data['price'].mean():.2f}\")\n",
        "    print(f\"Median order value: ${purchase_data['price'].median():.2f}\")\n",
        "    print(f\"Total revenue: ${purchase_data['price'].sum():,.2f}\")\n",
        "    \n",
        "    # Price distribution\n",
        "    plt.figure(figsize=(12, 6))\n",
        "    plt.hist(purchase_data['price'], bins=50, color='green', alpha=0.7, edgecolor='black')\n",
        "    plt.title('Purchase Price Distribution', fontsize=14, fontweight='bold')\n",
        "    plt.xlabel('Price ($)')\n",
        "    plt.ylabel('Frequency')\n",
        "    plt.yscale('log')\n",
        "    plt.grid(alpha=0.3)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Data Aggregation\n",
        "\n",
        "**Justification**: Aggregate event-level data to customer-level features. This transformation is essential because:\n",
        "- Clustering operates on customers, not individual events\n",
        "- We need to capture each customer's overall behavior pattern\n",
        "- Features include: event counts, spending metrics, engagement levels, temporal patterns, diversity metrics\n",
        "\n",
        "The aggregation creates one row per customer with comprehensive behavioral features.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Aggregate events to customer level\n",
        "print(\"Aggregating events to customer-level features...\")\n",
        "print(\"This may take a few minutes...\")\n",
        "\n",
        "customer_df = aggregate_data(df, include_rfm=True)\n",
        "\n",
        "print(f\"\\n✓ Aggregation completed\")\n",
        "print(f\"  - Original events: {len(df):,}\")\n",
        "print(f\"  - Unique customers: {len(customer_df):,}\")\n",
        "print(f\"  - Features created: {len(customer_df.columns)}\")\n",
        "\n",
        "print(f\"\\nCustomer-level features:\")\n",
        "print(list(customer_df.columns))\n",
        "\n",
        "print(f\"\\nFirst few rows:\")\n",
        "customer_df.head()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Summary statistics of customer features\n",
        "print(\"=== Customer Feature Statistics ===\")\n",
        "print(customer_df.describe())\n",
        "\n",
        "# Visualize key customer metrics\n",
        "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
        "\n",
        "# Total events distribution\n",
        "customer_df['total_events'].hist(bins=50, ax=axes[0, 0], color='steelblue', alpha=0.7)\n",
        "axes[0, 0].set_title('Total Events per Customer', fontweight='bold')\n",
        "axes[0, 0].set_xlabel('Total Events')\n",
        "axes[0, 0].set_ylabel('Frequency')\n",
        "axes[0, 0].set_yscale('log')\n",
        "\n",
        "# Purchase count distribution\n",
        "customer_df['purchase_count'].hist(bins=50, ax=axes[0, 1], color='green', alpha=0.7)\n",
        "axes[0, 1].set_title('Purchase Count per Customer', fontweight='bold')\n",
        "axes[0, 1].set_xlabel('Purchase Count')\n",
        "axes[0, 1].set_ylabel('Frequency')\n",
        "axes[0, 1].set_yscale('log')\n",
        "\n",
        "# Total spending distribution (for customers with purchases)\n",
        "spending_customers = customer_df[customer_df['total_spending'] > 0]\n",
        "if len(spending_customers) > 0:\n",
        "    spending_customers['total_spending'].hist(bins=50, ax=axes[1, 0], color='orange', alpha=0.7)\n",
        "    axes[1, 0].set_title('Total Spending per Customer (Purchasers)', fontweight='bold')\n",
        "    axes[1, 0].set_xlabel('Total Spending ($)')\n",
        "    axes[1, 0].set_ylabel('Frequency')\n",
        "    axes[1, 0].set_yscale('log')\n",
        "\n",
        "# Conversion rate distribution\n",
        "customer_df['purchase_conversion_rate'].hist(bins=50, ax=axes[1, 1], color='purple', alpha=0.7)\n",
        "axes[1, 1].set_title('Purchase Conversion Rate', fontweight='bold')\n",
        "axes[1, 1].set_xlabel('Conversion Rate')\n",
        "axes[1, 1].set_ylabel('Frequency')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Feature Engineering\n",
        "\n",
        "**Justification**: Create advanced features that capture customer behavior patterns:\n",
        "- **RFM Features**: Recency, Frequency, Monetary scores for classic customer segmentation\n",
        "- **Behavioral Features**: Engagement intensity, loyalty scores, activity levels\n",
        "- **Temporal Features**: Peak activity hours, session patterns\n",
        "- **Engagement Features**: Exploration vs focus, diversity metrics\n",
        "- **Price Sensitivity**: Price preference, tolerance, sensitivity metrics\n",
        "\n",
        "These features help the clustering algorithm identify distinct customer segments with meaningful behavioral differences.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Engineer additional features\n",
        "print(\"Engineering additional features...\")\n",
        "\n",
        "engineered_df = engineer_features(\n",
        "    customer_df,\n",
        "    include_behavioral=True,\n",
        "    include_temporal=True,\n",
        "    include_engagement=True,\n",
        "    include_price=True\n",
        ")\n",
        "\n",
        "print(f\"\\n✓ Feature engineering completed\")\n",
        "print(f\"  - Original features: {len(customer_df.columns)}\")\n",
        "print(f\"  - Engineered features: {len(engineered_df.columns)}\")\n",
        "print(f\"  - New features added: {len(engineered_df.columns) - len(customer_df.columns)}\")\n",
        "\n",
        "# Show new features\n",
        "new_features = [col for col in engineered_df.columns if col not in customer_df.columns]\n",
        "print(f\"\\nNew features created: {new_features}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Feature Selection\n",
        "\n",
        "**Justification**: Select only numeric features suitable for clustering. We exclude:\n",
        "- `user_id` (identifier, not a feature)\n",
        "- Categorical variables (will be handled separately if needed)\n",
        "- Features with infinite or invalid values\n",
        "\n",
        "This ensures the clustering algorithm receives clean, numeric input.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Select features for clustering\n",
        "print(\"Selecting features for clustering...\")\n",
        "\n",
        "features_df = select_features_for_clustering(engineered_df, exclude_cols=[\"user_id\"])\n",
        "\n",
        "print(f\"\\n✓ Feature selection completed\")\n",
        "print(f\"  - Selected features: {len(features_df.columns)}\")\n",
        "print(f\"  - Features: {list(features_df.columns)[:10]}...\")  # Show first 10\n",
        "\n",
        "# Check for infinite or invalid values\n",
        "print(f\"\\n=== Data Quality Check ===\")\n",
        "print(f\"Rows with infinite values: {(np.isinf(features_df).any(axis=1)).sum()}\")\n",
        "print(f\"Rows with NaN values: {features_df.isnull().any(axis=1).sum()}\")\n",
        "\n",
        "# Replace infinite values with NaN, then fill\n",
        "features_df = features_df.replace([np.inf, -np.inf], np.nan)\n",
        "features_df = features_df.fillna(features_df.median())\n",
        "\n",
        "print(f\"✓ Data cleaned\")\n",
        "print(f\"  - Final shape: {features_df.shape}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Preprocessing\n",
        "\n",
        "**Justification**: Preprocess data for clustering:\n",
        "- **Outlier Treatment**: Use IQR method to cap extreme values that could distort clusters\n",
        "- **Scaling**: Apply RobustScaler (more robust to outliers than StandardScaler) to normalize features\n",
        "- **Feature Selection**: Remove highly correlated features and low-variance features\n",
        "- **Optional PCA**: Can reduce dimensionality while retaining most variance\n",
        "\n",
        "Preprocessing ensures features are on similar scales and clustering isn't dominated by features with large values.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initialize and fit preprocessor\n",
        "print(\"Preprocessing data...\")\n",
        "print(f\"  - Scaling method: {PREPROCESSING_CONFIG.get('scaling_method', 'robust')}\")\n",
        "print(f\"  - Correlation threshold: {PREPROCESSING_CONFIG.get('correlation_threshold', 0.95)}\")\n",
        "print(f\"  - Variance threshold: {PREPROCESSING_CONFIG.get('variance_threshold', 0.01)}\")\n",
        "\n",
        "preprocessor = ClusteringPreprocessor(\n",
        "    scaling_method=PREPROCESSING_CONFIG.get(\"scaling_method\", \"robust\"),\n",
        "    use_pca=PREPROCESSING_CONFIG.get(\"use_pca\", False),\n",
        "    correlation_threshold=PREPROCESSING_CONFIG.get(\"correlation_threshold\", 0.95),\n",
        "    variance_threshold=PREPROCESSING_CONFIG.get(\"variance_threshold\", 0.01),\n",
        "    outlier_iqr_multiplier=FEATURE_CONFIG.get(\"outlier_iqr_multiplier\", 1.5)\n",
        ")\n",
        "\n",
        "X_processed = preprocessor.fit_transform(features_df)\n",
        "\n",
        "print(f\"\\n✓ Preprocessing completed\")\n",
        "print(f\"  - Original features: {features_df.shape[1]}\")\n",
        "print(f\"  - Processed features: {X_processed.shape[1]}\")\n",
        "print(f\"  - Features removed: {features_df.shape[1] - X_processed.shape[1]}\")\n",
        "\n",
        "# Save preprocessor\n",
        "preprocessor.save(MODEL_PATHS[\"preprocessor\"])\n",
        "joblib.dump(preprocessor.feature_names_, MODEL_PATHS[\"feature_names\"])\n",
        "print(f\"  - Preprocessor saved to: {MODEL_PATHS['preprocessor']}\")\n",
        "\n",
        "# Visualize feature distributions before and after scaling\n",
        "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
        "\n",
        "# Before scaling (sample features)\n",
        "sample_features = features_df.select_dtypes(include=[np.number]).iloc[:, :5]\n",
        "sample_features.boxplot(ax=axes[0])\n",
        "axes[0].set_title('Feature Distributions (Before Scaling)', fontweight='bold')\n",
        "axes[0].set_ylabel('Value')\n",
        "axes[0].tick_params(axis='x', rotation=45)\n",
        "\n",
        "# After scaling\n",
        "X_processed_df = pd.DataFrame(X_processed, columns=preprocessor.feature_names_)\n",
        "X_processed_df.iloc[:, :5].boxplot(ax=axes[1])\n",
        "axes[1].set_title('Feature Distributions (After Scaling)', fontweight='bold')\n",
        "axes[1].set_ylabel('Scaled Value')\n",
        "axes[1].tick_params(axis='x', rotation=45)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9. Optimal Cluster Selection\n",
        "\n",
        "**Justification**: Determine the optimal number of clusters using multiple methods:\n",
        "- **Elbow Method**: Find the \"elbow\" where inertia (within-cluster SS) stops decreasing significantly\n",
        "- **Silhouette Score**: Higher is better; measures how well-separated clusters are\n",
        "- **Davies-Bouldin Index**: Lower is better; measures average similarity between clusters\n",
        "- **Calinski-Harabasz Index**: Higher is better; ratio of between-cluster to within-cluster variance\n",
        "\n",
        "We use Silhouette Score as the primary method as it provides a good balance between cluster separation and cohesion.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Determine optimal number of clusters\n",
        "print(\"Determining optimal number of clusters...\")\n",
        "print(f\"Testing K values: {KMEANS_CONFIG.get('n_clusters_range', list(range(2, 11)))}\")\n",
        "\n",
        "selector = ClusterSelector(\n",
        "    n_clusters_range=KMEANS_CONFIG.get(\"n_clusters_range\", list(range(2, 11))),\n",
        "    random_state=KMEANS_CONFIG.get(\"random_state\", 42),\n",
        "    n_init=KMEANS_CONFIG.get(\"n_init\", 10)\n",
        ")\n",
        "\n",
        "# Evaluate all methods\n",
        "comparison_df = selector.evaluate_all_methods(X_processed)\n",
        "\n",
        "print(f\"\\n✓ Cluster selection evaluation completed\")\n",
        "print(\"\\nComparison of methods:\")\n",
        "print(comparison_df)\n",
        "\n",
        "# Select optimal K using silhouette score\n",
        "optimal_k = selector.select_optimal_k(X_processed, method=\"silhouette\")\n",
        "print(f\"\\n✓ Optimal number of clusters: {optimal_k} (selected using Silhouette Score)\")\n",
        "\n",
        "# Visualize cluster selection metrics\n",
        "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
        "\n",
        "# Elbow curve\n",
        "if \"elbow\" in selector.scores_:\n",
        "    elbow_scores = selector.scores_[\"elbow\"]\n",
        "    k_values = sorted(elbow_scores.keys())\n",
        "    inertias = [elbow_scores[k] for k in k_values]\n",
        "    plot_elbow_curve(k_values, inertias)\n",
        "\n",
        "# Silhouette scores\n",
        "if \"silhouette\" in selector.scores_:\n",
        "    silhouette_scores = selector.scores_[\"silhouette\"]\n",
        "    k_values = sorted([k for k in silhouette_scores.keys() if silhouette_scores[k] > -1])\n",
        "    scores = [silhouette_scores[k] for k in k_values]\n",
        "    plot_silhouette_scores(k_values, scores)\n",
        "\n",
        "# Davies-Bouldin scores\n",
        "if \"davies_bouldin\" in selector.scores_:\n",
        "    db_scores = selector.scores_[\"davies_bouldin\"]\n",
        "    k_values = sorted([k for k in db_scores.keys() if db_scores[k] < float(\"inf\")])\n",
        "    scores = [db_scores[k] for k in k_values]\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.plot(k_values, scores, 'ro-', linewidth=2, markersize=8)\n",
        "    plt.xlabel(\"Number of Clusters (K)\", fontsize=12)\n",
        "    plt.ylabel(\"Davies-Bouldin Index\", fontsize=12)\n",
        "    plt.title(\"Davies-Bouldin Index for Different K Values\", fontsize=14, fontweight=\"bold\")\n",
        "    plt.grid(alpha=0.3)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# Save comparison\n",
        "comparison_path = DIRECTORIES[\"reports\"] / \"cluster_selection_comparison.csv\"\n",
        "comparison_df.to_csv(comparison_path, index=False)\n",
        "print(f\"\\n✓ Comparison saved to: {comparison_path}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Train K-means clustering model\n",
        "print(f\"Training K-means clustering with {optimal_k} clusters...\")\n",
        "\n",
        "trainer = ClusteringTrainer(\n",
        "    n_clusters=optimal_k,\n",
        "    algorithm=\"kmeans\",\n",
        "    random_state=KMEANS_CONFIG.get(\"random_state\", 42)\n",
        ")\n",
        "\n",
        "trainer.fit(X_processed)\n",
        "\n",
        "print(f\"\\n✓ Model training completed\")\n",
        "print(f\"  - Number of clusters: {len(np.unique(trainer.labels_))}\")\n",
        "print(f\"  - Cluster sizes: {dict(zip(*np.unique(trainer.labels_, return_counts=True)))}\")\n",
        "\n",
        "# Save model\n",
        "trainer.save(MODEL_PATHS[\"clusterer\"])\n",
        "print(f\"  - Model saved to: {MODEL_PATHS['clusterer']}\")\n",
        "\n",
        "# Add cluster labels to customer dataframe for analysis\n",
        "engineered_df['cluster'] = trainer.labels_\n",
        "features_df['cluster'] = trainer.labels_\n",
        "\n",
        "print(f\"\\nCluster distribution:\")\n",
        "print(engineered_df['cluster'].value_counts().sort_index())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 11. Cluster Evaluation\n",
        "\n",
        "**Justification**: Evaluate clustering quality using internal metrics:\n",
        "- **Silhouette Score**: Measures how similar customers are to their own cluster vs other clusters (range: -1 to 1, higher is better)\n",
        "- **Davies-Bouldin Index**: Average similarity ratio of clusters (lower is better)\n",
        "- **Calinski-Harabasz Index**: Ratio of between-cluster to within-cluster variance (higher is better)\n",
        "\n",
        "These metrics help validate that clusters are well-separated and internally cohesive.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Evaluate clustering results\n",
        "print(\"Evaluating clustering results...\")\n",
        "\n",
        "evaluator = ClusterEvaluator(\n",
        "    model=trainer.model,\n",
        "    preprocessor=preprocessor,\n",
        "    save_plots=True\n",
        ")\n",
        "\n",
        "metrics = evaluator.evaluate(X_processed, trainer.labels_, feature_names=preprocessor.feature_names_)\n",
        "\n",
        "print(f\"\\n✓ Evaluation completed\")\n",
        "print(f\"\\n=== Evaluation Metrics ===\")\n",
        "print(f\"Silhouette Score: {metrics.get('silhouette_score', 'N/A'):.4f}\")\n",
        "print(f\"Davies-Bouldin Index: {metrics.get('davies_bouldin', 'N/A'):.4f}\")\n",
        "print(f\"Calinski-Harabasz Index: {metrics.get('calinski_harabasz', 'N/A'):.4f}\")\n",
        "if 'inertia' in metrics:\n",
        "    print(f\"Inertia: {metrics['inertia']:.2f}\")\n",
        "print(f\"\\nCluster sizes:\")\n",
        "for cluster, size in sorted(metrics['cluster_sizes'].items()):\n",
        "    print(f\"  Cluster {int(cluster)}: {size} customers ({size/len(trainer.labels_)*100:.1f}%)\")\n",
        "\n",
        "# Generate cluster profiles\n",
        "profiles = evaluator.generate_cluster_profiles(\n",
        "    features_df,\n",
        "    trainer.labels_,\n",
        "    feature_names=features_df.columns.tolist()\n",
        ")\n",
        "\n",
        "print(f\"\\n✓ Cluster profiles generated\")\n",
        "print(f\"\\nCluster Profiles (mean feature values):\")\n",
        "print(profiles.head(10))  # Show first 10 features\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 12. Cluster Analysis and Visualization\n",
        "\n",
        "**Justification**: Visualize and interpret clusters to understand:\n",
        "- How clusters are separated in feature space (PCA visualization)\n",
        "- Cluster size distribution\n",
        "- Feature characteristics of each cluster\n",
        "- Business interpretation of each segment\n",
        "\n",
        "This helps translate clustering results into actionable business insights.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize clusters in PCA space\n",
        "print(\"Creating visualizations...\")\n",
        "\n",
        "# 2D PCA visualization\n",
        "plot_pca_clusters(\n",
        "    X_processed,\n",
        "    trainer.labels_,\n",
        "    n_components=2,\n",
        "    save_path=DIRECTORIES[\"reports\"] / \"pca_clusters_2d.png\",\n",
        "    title=f\"Customer Clusters in 2D PCA Space (K={optimal_k})\"\n",
        ")\n",
        "\n",
        "# Cluster distribution\n",
        "evaluator.plot_cluster_distribution(\n",
        "    trainer.labels_,\n",
        "    save_path=DIRECTORIES[\"reports\"] / \"cluster_distribution.png\"\n",
        ")\n",
        "\n",
        "# Cluster profiles heatmap\n",
        "evaluator.plot_cluster_profiles(\n",
        "    profiles,\n",
        "    save_path=DIRECTORIES[\"reports\"] / \"cluster_profiles.png\",\n",
        "    top_n_features=15\n",
        ")\n",
        "\n",
        "# Feature distributions by cluster\n",
        "key_features = ['total_events', 'purchase_count', 'total_spending', 'purchase_conversion_rate', \n",
        "                'avg_order_value', 'unique_products_viewed']\n",
        "available_features = [f for f in key_features if f in features_df.columns]\n",
        "\n",
        "if len(available_features) > 0:\n",
        "    plot_feature_distributions_by_cluster(\n",
        "        features_df,\n",
        "        trainer.labels_,\n",
        "        available_features,\n",
        "        n_features=len(available_features),\n",
        "        save_path=DIRECTORIES[\"reports\"] / \"feature_distributions_by_cluster.png\"\n",
        "    )\n",
        "\n",
        "print(\"✓ Visualizations created and saved\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Detailed cluster analysis\n",
        "print(\"\\n=== Detailed Cluster Analysis ===\")\n",
        "\n",
        "for cluster_id in sorted(engineered_df['cluster'].unique()):\n",
        "    cluster_data = engineered_df[engineered_df['cluster'] == cluster_id]\n",
        "    print(f\"\\n--- Cluster {int(cluster_id)} ({len(cluster_data)} customers, {len(cluster_data)/len(engineered_df)*100:.1f}%) ---\")\n",
        "    \n",
        "    # Key metrics\n",
        "    print(f\"  Average Events: {cluster_data['total_events'].mean():.1f}\")\n",
        "    print(f\"  Average Purchases: {cluster_data['purchase_count'].mean():.1f}\")\n",
        "    if cluster_data['total_spending'].sum() > 0:\n",
        "        print(f\"  Average Spending: ${cluster_data[cluster_data['total_spending'] > 0]['total_spending'].mean():.2f}\")\n",
        "        print(f\"  Total Revenue: ${cluster_data['total_spending'].sum():,.2f}\")\n",
        "    print(f\"  Conversion Rate: {cluster_data['purchase_conversion_rate'].mean():.3f}\")\n",
        "    print(f\"  Unique Products: {cluster_data['unique_products_viewed'].mean():.1f}\")\n",
        "    print(f\"  Unique Categories: {cluster_data['unique_categories'].mean():.1f}\")\n",
        "    \n",
        "    # Segment characteristics\n",
        "    if 'recency_score' in cluster_data.columns:\n",
        "        print(f\"  RFM - Recency Score: {cluster_data['recency_score'].mean():.1f}\")\n",
        "        print(f\"  RFM - Frequency Score: {cluster_data['frequency_score'].mean():.1f}\")\n",
        "        print(f\"  RFM - Monetary Score: {cluster_data['monetary_score'].mean():.1f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 13. Generate Evaluation Report\n",
        "\n",
        "**Justification**: Create a comprehensive markdown report summarizing:\n",
        "- Model configuration and parameters\n",
        "- Evaluation metrics\n",
        "- Cluster statistics and profiles\n",
        "- Business interpretations and recommendations\n",
        "\n",
        "This report serves as documentation for stakeholders and future reference.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Generate and save evaluation report\n",
        "print(\"Generating evaluation report...\")\n",
        "\n",
        "model_info = {\n",
        "    \"Algorithm\": \"K-means\",\n",
        "    \"Number of Clusters\": optimal_k,\n",
        "    \"Number of Features\": X_processed.shape[1],\n",
        "    \"Number of Customers\": len(customer_df),\n",
        "    \"Selection Method\": \"Silhouette Score\",\n",
        "    \"Preprocessing\": f\"RobustScaler, Correlation threshold: {PREPROCESSING_CONFIG.get('correlation_threshold', 0.95)}\"\n",
        "}\n",
        "\n",
        "report = evaluator.generate_report(metrics, profiles, model_info)\n",
        "evaluator.save_report(report, DIRECTORIES[\"reports\"] / \"cluster_report.md\")\n",
        "\n",
        "print(f\"✓ Report generated and saved to: {DIRECTORIES['reports'] / 'cluster_report.md'}\")\n",
        "print(\"\\nReport preview:\")\n",
        "print(report[:500] + \"...\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 14. Summary and Business Insights\n",
        "\n",
        "**Justification**: Summarize key findings and provide actionable business recommendations based on the identified customer segments. This helps translate technical results into business value.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"=\" * 60)\n",
        "print(\"PIPELINE SUMMARY\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "print(f\"\\n✓ Successfully segmented {len(customer_df):,} customers into {optimal_k} clusters\")\n",
        "print(f\"✓ Clustering Quality (Silhouette Score): {metrics.get('silhouette_score', 'N/A'):.4f}\")\n",
        "\n",
        "print(f\"\\n=== Key Findings ===\")\n",
        "print(f\"1. Data Processed: {len(df):,} events from {customer_df['user_id'].nunique():,} customers\")\n",
        "print(f\"2. Features Created: {len(engineered_df.columns)} customer-level features\")\n",
        "print(f\"3. Optimal Clusters: {optimal_k} (determined using Silhouette Score)\")\n",
        "print(f\"4. Cluster Quality: {'Excellent' if metrics.get('silhouette_score', 0) > 0.5 else 'Good' if metrics.get('silhouette_score', 0) > 0.3 else 'Fair'}\")\n",
        "\n",
        "print(f\"\\n=== Business Recommendations ===\")\n",
        "print(\"1. **High-Value Customers**: Focus retention efforts on clusters with high spending and purchase frequency\")\n",
        "print(\"2. **At-Risk Customers**: Identify clusters with declining engagement and implement re-engagement campaigns\")\n",
        "print(\"3. **New Customer Onboarding**: Tailor onboarding for clusters with low engagement but potential\")\n",
        "print(\"4. **Personalization**: Use cluster profiles to personalize product recommendations and marketing messages\")\n",
        "print(\"5. **Pricing Strategy**: Adjust pricing for clusters with high price sensitivity\")\n",
        "\n",
        "print(f\"\\n=== Next Steps ===\")\n",
        "print(\"1. Deploy clustering model for real-time customer segmentation\")\n",
        "print(\"2. Integrate with marketing automation for personalized campaigns\")\n",
        "print(\"3. Monitor cluster evolution over time\")\n",
        "print(\"4. A/B test marketing strategies by cluster\")\n",
        "print(\"5. Refine clusters with additional features or domain knowledge\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"Pipeline completed successfully!\")\n",
        "print(\"=\" * 60)\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
